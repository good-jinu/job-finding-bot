from typing import List, Optional
from browser_use.llm import ChatGoogle
from browser_use import Agent, Controller
from browser_use.browser import BrowserProfile
from dotenv import load_dotenv
from src.core.schemas.job_posting import JobPosting, JobPostingList
from src.core.database.job_postings import save_job_postings, update_content_doc
from src.core.file_storage.file_manager import FileManager
from src.core.file_storage.paths import FileStoragePaths
from src.core.services.utils.generate_random_data import generate_random_string
from urllib.parse import quote

load_dotenv()

# Browser profile and LLM configuration
profile = BrowserProfile(
  stealth=True,
  wait_between_actions=5,
  user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
)
llm = ChatGoogle(model="gemini-2.5-flash")

# File storage initialization
file_paths = FileStoragePaths()
file_manager = FileManager(file_paths)


async def get_job_search_urls(keyword: str) -> List[str]:
  """ê²€ìƒ‰ í‚¤ì›Œë“œì— ëŒ€í•œ ì±„ìš© ì‚¬ì´íŠ¸ URL ëª©ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤."""
  encoded_keyword = quote(keyword)
  return [
    f"https://www.jobkorea.co.kr/Search/?stext={encoded_keyword}",
    f"https://www.wanted.co.kr/search?query={encoded_keyword}&tab=overview",
    f"https://www.saramin.co.kr/zf_user/search?search_area=main&search_done=y&search_optional_item=n&searchType=search&searchword={encoded_keyword}",
  ]


async def collect_job_postings(search_page_url: str) -> List[JobPosting]:
  """
  (ëª©ë¡ í˜ì´ì§€ìš©) ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ì—ì„œ 'JobPosting' ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
  """
  controller = Controller(output_model=JobPostingList)
  task = (
    f"í˜„ì¬ í˜ì´ì§€({search_page_url})ì—ì„œ ì±„ìš© ê³µê³  ëª©ë¡ì„ ì¶”ì¶œí•´ì¤˜. "
    "ê° ê³µê³ ë§ˆë‹¤ 'title', 'company', 'location', ìƒì„¸ ì •ë³´ 'url'ì„ í¬í•¨í•œ 'JobPosting' ê°ì²´ë¥¼ ìƒì„±í•´ì¤˜. "
    "ëª©ë¡ í˜ì´ì§€ì—ì„œ ìƒì„¸ ì„¤ëª…(description)ì´ë‚˜ ë“±ë¡ì¼(posted_at)ì„ ì•Œ ìˆ˜ ì—†ë‹¤ë©´ ë¹„ì›Œë‘¬ë„ ê´œì°®ì•„. "
    "content_docì€ ë¹„ì›Œë‘¬."
  )

  agent = Agent(
    task=task,
    llm=llm,
    profile=profile,
    controller=controller,
    initial_actions=[{"go_to_url": {"url": search_page_url}}],
  )
  print(f"Collecting job postings from: {search_page_url}...")
  result_json = await agent.run()

  if not result_json:
    print(f"  -> No postings found from {search_page_url}")
    return []
  try:
    # ê²°ê³¼ë¥¼ JobPostingList ëª¨ë¸ë¡œ íŒŒì‹±í•˜ê³  ë‚´ë¶€ì˜ postings ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    parsed_data = JobPostingList.model_validate_json(result_json)
    print(f"  -> Collected {len(parsed_data.data_list)} postings.")
    return parsed_data.data_list
  except Exception as e:
    print(f"  -> Error parsing postings from {search_page_url}: {e}")
    return []


async def extract_job_detail_content(detail_url: str) -> Optional[str]:
  """
  ì±„ìš© ê³µê³  ìƒì„¸ í˜ì´ì§€ì—ì„œ ìƒì„¸ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì—¬ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
  """
  controller = Controller()
  task = (
    f"ì´ ì±„ìš© ê³µê³  í˜ì´ì§€({detail_url})ì˜ ëª¨ë“  ìƒì„¸ ì •ë³´ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì¤˜. "
    "í¬í•¨í•  ë‚´ìš©: ì§ë¬´ëª…, íšŒì‚¬ëª…, ê·¼ë¬´ì§€ì—­, ì±„ìš© ìƒì„¸ ë‚´ìš©, ìê²© ìš”ê±´, ìš°ëŒ€ ì‚¬í•­, "
    "ê·¼ë¬´ ì¡°ê±´, ë³µë¦¬í›„ìƒ, ì±„ìš© ì ˆì°¨ ë“± ëª¨ë“  ê´€ë ¨ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜."
  )

  agent = Agent(
    task=task,
    llm=llm,
    profile=profile,
    controller=controller,
    initial_actions=[{"go_to_url": {"url": detail_url}}],
  )
  print(f"Extracting detail content from: {detail_url}...")
  result = await agent.run()

  if result:
    print("  -> Successfully extracted content")
    return result
  else:
    print("  -> Failed to extract content")
    return None


async def collect_and_extract_job_postings(
  keyword: str = "í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì",
) -> List[JobPosting]:
  """
  ì£¼ì–´ì§„ í‚¤ì›Œë“œë¡œ ì—¬ëŸ¬ ì±„ìš© ì‚¬ì´íŠ¸ì—ì„œ ê³µê³  ëª©ë¡ì„ ìˆ˜ì§‘í•˜ê³ ,
  ê° ê³µê³ ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JobPosting ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
  """
  final_results: List[JobPosting] = []

  try:
    # Step 1: ê²€ìƒ‰í•  ì±„ìš© ì‚¬ì´íŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    search_urls = await get_job_search_urls(keyword)
    print(f"ğŸ” Found {len(search_urls)} search pages for keyword '{keyword}'.")
    print("-" * 30)

    # Step 2: ëª¨ë“  ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ JobPosting ê°ì²´ ëª©ë¡ ìˆ˜ì§‘
    initial_postings: List[JobPosting] = []
    for url in search_urls:
      try:
        postings = await collect_job_postings(url)
        initial_postings.extend(postings)
      except Exception as e:
        print(f"Error collecting postings from {url}: {e}")

    # ì¤‘ë³µ URLì„ ê°€ì§„ ê°ì²´ ì œê±°
    unique_postings = {p.url: p for p in initial_postings if p.url}.values()
    print(f"\nTotal unique job postings found from lists: {len(unique_postings)}")
    print("-" * 30)

    # Step 3: DBì— JobPosting ì €ì¥ (content_docì€ ë¹„ì–´ìˆìŒ)
    postings_to_save = list(unique_postings)[:5]  # ë°ëª¨ë¥¼ ìœ„í•´ 5ê°œë§Œ ì‹¤í–‰
    save_job_postings(postings_to_save)
    print(f"Saved {len(postings_to_save)} job postings to database")

    # Step 4: ê° ê³µê³ ì˜ ìƒì„¸ URLë¡œ ì ‘ì†í•˜ì—¬ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ë° íŒŒì¼ ì €ì¥
    for posting in postings_to_save:
      try:
        # ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
        detail_content = await extract_job_detail_content(posting.url)

        if detail_content:
          # íŒŒì¼ëª… ìƒì„±
          random_str = generate_random_string()
          filename = f"{posting.id}_{random_str}.md"

          # íŒŒì¼ ì €ì¥
          file_path = file_paths.get_job_content_path(filename)
          success = await file_manager.write_file_async(file_path, detail_content)

          if success:
            # content_doc ì—…ë°ì´íŠ¸
            if posting.id:
              update_content_doc(posting.id, filename)
              posting.content_doc = filename
              print(f"  -> Saved content to {filename} and updated database")
            else:
              print("  -> Warning: Job posting has no ID, cannot update content_doc")
          else:
            print(f"  -> Failed to save content file for {posting.url}")
        else:
          print(f"  -> No content extracted for {posting.url}")

        final_results.append(posting)

      except Exception as e:
        print(f"Error processing detail for {posting.url}: {e}")
        final_results.append(posting)  # ì—ëŸ¬ê°€ ìˆì–´ë„ ê¸°ë³¸ ì •ë³´ëŠ” í¬í•¨

    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)
    print(f"ğŸ‰ Total job postings processed: {len(final_results)}")
    print("=" * 50)

    for i, job in enumerate(final_results, 1):
      print(f"\n--- Job Posting #{i} ---")
      print(f"  ID: {job.id}")
      print(f"  URL: {job.url}")
      print(f"  Title: {job.title}")
      print(f"  Company: {job.company}")
      print(f"  Location: {job.location}")
      print(f"  Posted At: {job.posted_at or 'N/A'}")
      print(f"  Content Doc: {job.content_doc or 'N/A'}")
      if job.description:
        print(f"  Description: {job.description[:100].strip()}...")

  except Exception as e:
    print(f"An error occurred in the job postings collection process: {e}")

  return final_results
