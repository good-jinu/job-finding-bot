from typing import List, Optional
from browser_use.llm import ChatGoogle
from browser_use import Agent
from browser_use.browser import BrowserProfile
from dotenv import load_dotenv
from src.core.schemas.job_posting import JobPosting, JobPostingList
from src.core.database.job_postings import save_job_postings, update_content_doc
from src.core.file_storage.file_manager import FileManager
from src.core.file_storage.paths import FileStoragePaths
from src.core.services.utils.generate_random_data import generate_random_string
from urllib.parse import quote
from src.core.llm.providers import get_structured_output_model

load_dotenv()

# Browser profile and LLM configuration
profile = BrowserProfile(
  stealth=True,
  wait_between_actions=5,
  user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
)
# This LLM is used by the browser agent for navigation and simple extraction tasks.
llm = ChatGoogle(model="gemini-2.5-flash")

# File storage initialization
file_paths = FileStoragePaths()
file_manager = FileManager(file_paths)


async def get_job_search_urls(keyword: str) -> List[str]:
  """ê²€ìƒ‰ í‚¤ì›Œë“œì— ëŒ€í•œ ì±„ìš© ì‚¬ì´íŠ¸ URL ëª©ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤."""
  encoded_keyword = quote(keyword)
  return [
    f"https://www.jobkorea.co.kr/Search/?stext={encoded_keyword}",
    # f"https://www.wanted.co.kr/search?query={encoded_keyword}&tab=overview",
    # f"https://www.saramin.co.kr/zf_user/search?search_area=main&search_done=y&search_optional_item=n&searchType=search&searchword={encoded_keyword}",
  ]


async def collect_job_postings(search_page_url: str) -> List[JobPosting]:
  """
  (ëª©ë¡ í˜ì´ì§€ìš©) ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ì—ì„œ 'JobPosting' ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
  ë¸Œë¼ìš°ì €ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìŠ¤í¬ë©í•œ í›„ LLMì„ ì‚¬ìš©í•˜ì—¬ JobPostingListë¡œ êµ¬ì¡°í™”í•©ë‹ˆë‹¤.
  """
  task = (
    f"í˜„ì¬ í˜ì´ì§€({search_page_url})ì—ì„œ ì±„ìš© ê³µê³  ëª©ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ì¶”ì¶œí•´ì¤˜. "
    "ê° ê³µê³ ë§ˆë‹¤ 'title', 'company', 'location', ìƒì„¸ ì •ë³´ 'url'ì„ í¬í•¨í•´ì•¼ í•´. "
    "ìµœëŒ€í•œ ë§ì€ ê³µê³ ë¥¼ ìˆ˜ì§‘í•˜ê¸° ìœ„í•´ í•„ìš”í•˜ë‹¤ë©´ ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ì¤˜. ê·¸ëŸ°ë° ë”± í•œí˜ì´ì§€ë§Œ ìŠ¤í¬ë©í•´ì¤˜. "
  )

  agent = Agent(
    task=task,
    llm=llm,
    profile=profile,
    initial_actions=[{"go_to_url": {"url": search_page_url}}],
  )
  print(f"Collecting job postings from: {search_page_url}...")

  try:
    history = await agent.run()
    scraped_text = history.final_result()

    if not scraped_text or not scraped_text.strip():
      print("  -> Agentê°€ ë¹„ì–´ìˆëŠ” ìµœì¢… ê²°ê³¼ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
      return []

    # LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”í•©ë‹ˆë‹¤.
    structured_llm = get_structured_output_model().with_structured_output(
      JobPostingList
    )

    prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì±„ìš© ê³µê³  ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜.
ê° ê³µê³ ëŠ” 'title', 'company', 'location', 'url' í•„ë“œë¥¼ í¬í•¨í•´ì•¼ í•´.
ê²°ê³¼ëŠ” 'jobs'ë¼ëŠ” í‚¤ë¥¼ ê°€ì§„ JSON ê°ì²´ì—¬ì•¼ í•´.

---
{scraped_text}
---
"""

    job_posting_list = await structured_llm.ainvoke(prompt)

    if job_posting_list and hasattr(job_posting_list, "jobs"):
      print(
        f"  -> LLMì´ {len(job_posting_list.jobs)}ê°œì˜ ê³µê³ ë¥¼ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¡°í™”í–ˆìŠµë‹ˆë‹¤."
      )
      return job_posting_list.jobs
    else:
      print("  -> LLMì´ í…ìŠ¤íŠ¸ì—ì„œ ê³µê³ ë¥¼ êµ¬ì¡°í™”í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
      return []

  except Exception as e:
    print(f"  -> ê³µê³  ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return []


async def extract_and_structure_job_detail(detail_url: str) -> Optional[JobPosting]:
  """
  ì±„ìš© ê³µê³  ìƒì„¸ í˜ì´ì§€ì—ì„œ ìƒì„¸ ë‚´ìš©ì„ ì¶”ì¶œí•˜ê³  JobPosting ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
  """
  task = (
    f"ì´ ì±„ìš© ê³µê³  í˜ì´ì§€({detail_url})ì˜ ëª¨ë“  ìƒì„¸ ì •ë³´ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì¤˜. "
    "í¬í•¨í•  ë‚´ìš©: ì§ë¬´ëª…, íšŒì‚¬ëª…, ê·¼ë¬´ì§€ì—­, ì±„ìš© ìƒì„¸ ë‚´ìš©, ìê²© ìš”ê±´, ìš°ëŒ€ ì‚¬í•­, "
    "ê·¼ë¬´ ì¡°ê±´, ë³µë¦¬í›„ìƒ, ì±„ìš© ì ˆì°¨ ë“± ëª¨ë“  ê´€ë ¨ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜."
  )

  agent = Agent(
    task=task,
    llm=llm,
    profile=profile,
    initial_actions=[{"go_to_url": {"url": detail_url}}],
  )
  print(f"Extracting detail content from: {detail_url}...")

  try:
    history = await agent.run()
    scraped_text = history.final_result()

    if not scraped_text or not scraped_text.strip():
      print("  -> ì—ì´ì „íŠ¸ ì‘ë‹µì—ì„œ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
      return None

    print("  -> ì„±ê³µì ìœ¼ë¡œ ìƒì„¸ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤. ì´ì œ êµ¬ì¡°í™”í•©ë‹ˆë‹¤...")

    # LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”í•©ë‹ˆë‹¤.
    structured_llm = get_structured_output_model().with_structured_output(JobPosting)

    prompt = f"""ë‹¤ìŒ ì±„ìš© ê³µê³  í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•´ì„œ JobPosting ê°ì²´ì— ë§ëŠ” JSONìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜.
- 'url' í•„ë“œëŠ” '{detail_url}' ë¡œ ì„¤ì •í•´ì¤˜.
- 'title', 'company', 'location' í•„ë“œë¥¼ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œí•´ì¤˜.
- 'description' í•„ë“œì—ëŠ” ì „ì²´ ê³µê³  ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì„œ ë„£ì–´ì¤˜.
- 'posted_at' í•„ë“œëŠ” ë“±ë¡ì¼ ë˜ëŠ” ë§ˆê°ì¼ì„ 'YYYY-MM-DD' í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ê±°ë‚˜, 'ìƒì‹œì±„ìš©' ê°™ì€ í…ìŠ¤íŠ¸ë¥¼ ë„£ì–´ì¤˜. ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ë¹„ì›Œë‘¬.
- 'id', 'created_at', 'updated_at', 'content_doc' í•„ë“œëŠ” ë¹„ì›Œë‘¬.

---
{scraped_text}
---
"""

    job_posting = await structured_llm.ainvoke(prompt)

    if job_posting:
      print("  -> ì„±ê³µì ìœ¼ë¡œ ìƒì„¸ ë‚´ìš©ì„ êµ¬ì¡°í™”í–ˆìŠµë‹ˆë‹¤.")
      return job_posting
    else:
      print("  -> LLMì´ í…ìŠ¤íŠ¸ì—ì„œ ê³µê³  ìƒì„¸ ì •ë³´ë¥¼ êµ¬ì¡°í™”í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
      return None

  except Exception as e:
    print(f"  -> ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ë° êµ¬ì¡°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return None


async def collect_and_extract_job_postings(
  keyword: str = "í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì",
) -> List[JobPosting]:
  """
  ì£¼ì–´ì§„ í‚¤ì›Œë“œë¡œ ì—¬ëŸ¬ ì±„ìš© ì‚¬ì´íŠ¸ì—ì„œ ê³µê³  ëª©ë¡ì„ ìˆ˜ì§‘í•˜ê³ ,
  ê° ê³µê³ ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JobPosting ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
  """
  final_results: List[JobPosting] = []

  try:
    # Step 1: ê²€ìƒ‰í•  ì±„ìš© ì‚¬ì´íŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    search_urls = await get_job_search_urls(keyword)
    print(f"ğŸ” '{keyword}' í‚¤ì›Œë“œë¡œ {len(search_urls)}ê°œì˜ ì±„ìš© ì‚¬ì´íŠ¸ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
    print("-" * 30)

    # Step 2: ëª¨ë“  ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ JobPosting ê°ì²´ ëª©ë¡ ìˆ˜ì§‘
    initial_postings: List[JobPosting] = []
    for url in search_urls:
      try:
        postings = await collect_job_postings(url)
        initial_postings.extend(postings)
      except Exception as e:
        print(f"{url} ì—ì„œ ê³µê³  ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

    # ì¤‘ë³µ URLì„ ê°€ì§„ ê°ì²´ ì œê±°
    unique_postings = list({p.url: p for p in initial_postings if p.url}.values())
    print(f"\nì´ {len(unique_postings)}ê°œì˜ ê³ ìœ í•œ ì±„ìš© ê³µê³ ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    print("-" * 30)

    if not unique_postings:
      print("ì²˜ë¦¬í•  ì±„ìš© ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
      return []

    # Step 3: DBì— JobPosting ì €ì¥ (ì´ˆê¸° ì •ë³´)
    # ë°ëª¨ë¥¼ ìœ„í•´ 5ê°œë§Œ ì‹¤í–‰ (ì‹¤ì œ ìš´ì˜ ì‹œ ì´ ë¶€ë¶„ì„ ì¡°ì ˆí•˜ì„¸ìš”)
    initial_postings_to_process = unique_postings[:5]
    saved_postings = save_job_postings(initial_postings_to_process)
    print(
      f"{len(saved_postings)}ê°œì˜ ì±„ìš© ê³µê³ ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê³  IDë¥¼ ë¶€ì—¬í–ˆìŠµë‹ˆë‹¤."
    )

    # Step 4: ê° ê³µê³ ì˜ ìƒì„¸ URLë¡œ ì ‘ì†í•˜ì—¬ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ, êµ¬ì¡°í™” ë° ì—…ë°ì´íŠ¸
    for posting in saved_postings:
      try:
        if not posting.id:
          print(
            f"  -> ê²½ê³ : {posting.url} ê³µê³ ê°€ ì €ì¥ í›„ IDê°€ ì—†ìŠµë‹ˆë‹¤. ìƒì„¸ ì •ë³´ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤."
          )
          final_results.append(posting)
          continue

        # ìƒì„¸ ì •ë³´ ì¶”ì¶œ ë° êµ¬ì¡°í™”
        detailed_posting = await extract_and_structure_job_detail(posting.url)

        if detailed_posting and detailed_posting.description:
          # íŒŒì¼ëª… ìƒì„±
          random_str = generate_random_string()
          filename = f"{posting.id}_{random_str}.md"

          # íŒŒì¼ ì €ì¥
          file_path = file_paths.get_job_content_path(filename)
          success = await file_manager.write_file_async(
            file_path, detailed_posting.description
          )

          if success:
            # content_doc DB ì—…ë°ì´íŠ¸
            update_content_doc(posting.id, filename)
            posting.content_doc = filename
            print(
              f"  -> ìƒì„¸ ë‚´ìš©ì„ {filename}ì— ì €ì¥í•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤."
            )

            # ì°¸ê³ : DBì— ì „ì²´ ìƒì„¸ë‚´ìš©(description, posted_at ë“±)ì„ ì—…ë°ì´íŠ¸í•˜ë ¤ë©´
            # `src/core/database/job_postings.py`ì— `update_job_posting(id, data)`ì™€ ê°™ì€
            # ë²”ìš© ì—…ë°ì´íŠ¸ í•¨ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬ëŠ” ë©”ëª¨ë¦¬ì˜ ê°ì²´ë§Œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
            posting.description = detailed_posting.description
            posting.posted_at = detailed_posting.posted_at
            posting.title = detailed_posting.title or posting.title
            posting.company = detailed_posting.company or posting.company
            posting.location = detailed_posting.location or posting.location

          else:
            print(f"  -> {posting.url}ì˜ ìƒì„¸ ë‚´ìš© íŒŒì¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        else:
          print(f"  -> {posting.url}ì˜ ìƒì„¸ ë‚´ìš©ì„ ì¶”ì¶œí•˜ê±°ë‚˜ êµ¬ì¡°í™”í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        final_results.append(posting)

      except Exception as e:
        print(f"{posting.url}ì˜ ìƒì„¸ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        final_results.append(posting)

    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)
    print(f"ğŸ‰ ì´ {len(final_results)}ê°œì˜ ì±„ìš© ê³µê³  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("=" * 50)

    for i, job in enumerate(final_results, 1):
      print(f"\n--- Job Posting #{i} ---")
      print(f"  ID: {job.id}")
      print(f"  URL: {job.url}")
      print(f"  Title: {job.title}")
      print(f"  Company: {job.company}")
      print(f"  Location: {job.location}")
      print(f"  Posted At: {job.posted_at or 'N/A'}")
      print(f"  Content Doc: {job.content_doc or 'N/A'}")
      if job.description:
        print(f"  Description: {job.description[:100].strip()}...")

  except Exception as e:
    print(f"ì±„ìš© ê³µê³  ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

  return final_results
